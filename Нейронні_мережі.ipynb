{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbLHTNfSclli"
   },
   "source": [
    "**Секція 1. Логістична регресія з нуля.**\n",
    "\n",
    "Будемо крок за кроком будувати модель лог регресії з нуля для передбачення, чи буде врожай більше за 80 яблук (задача подібна до лекційної, але на класифікацію).\n",
    "\n",
    "Давайте нагадаємо основні формули для логістичної регресії.\n",
    "\n",
    "### Функція гіпотези - обчислення передбачення у логістичній регресії:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(x W^T + b) = \\frac{1}{1 + e^{-(x W^T + b)}}\n",
    "$$\n",
    "\n",
    "Де:\n",
    "- $ \\hat{y} $ — це ймовірність \"позитивного\" класу.\n",
    "- $ x $ — це вектор (або матриця для набору прикладів) вхідних даних.\n",
    "- $ W $ — це вектор (або матриця) вагових коефіцієнтів моделі.\n",
    "- $ b $ — це зміщення (bias).\n",
    "- $ \\sigma(z) $ — це сигмоїдна функція активації.\n",
    "\n",
    "### Як обчислюється сигмоїдна функція:\n",
    "\n",
    "Сигмоїдна функція $ \\sigma(z) $ має вигляд:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Ця функція перетворює будь-яке дійсне значення $ z $ в інтервал від 0 до 1, що дозволяє інтерпретувати вихід як ймовірність для логістичної регресії.\n",
    "\n",
    "### Формула функції втрат для логістичної регресії (бінарна крос-ентропія):\n",
    "\n",
    "Функція втрат крос-ентропії оцінює, наскільки добре модель передбачає класи, порівнюючи передбачені ймовірності $ \\hat{y} $ із справжніми мітками $ y $. Формула наступна:\n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}) = - \\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n",
    "$$\n",
    "\n",
    "Де:\n",
    "- $ y $ — це справжнє значення (мітка класу, 0 або 1).\n",
    "- $ \\hat{y} $ — це передбачене значення (ймовірність).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtOYB-RHfc_r"
   },
   "source": [
    "1.\n",
    "Тут вже наведений код для ініціювання набору даних в форматі numpy. Перетворіть `inputs`, `targets` на `torch` тензори. Виведіть результат на екран."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "id": "3BNXSR-VdYKQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "id": "QLKZ77x4v_-v"
   },
   "outputs": [],
   "source": [
    "# Вхідні дані (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43],\n",
    "                   [91, 88, 64],\n",
    "                   [87, 134, 58],\n",
    "                   [102, 43, 37],\n",
    "                   [69, 96, 70]], dtype='float32')\n",
    "\n",
    "# Таргети (apples > 80)\n",
    "targets = np.array([[0],\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [1]], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "id": "KjoeaDrk6fO7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 73.,  67.,  43.],\n",
       "         [ 91.,  88.,  64.],\n",
       "         [ 87., 134.,  58.],\n",
       "         [102.,  43.,  37.],\n",
       "         [ 69.,  96.,  70.]]),\n",
       " tensor([[0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.]]))"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.tensor(inputs)\n",
    "targets = torch.tensor(targets)\n",
    "\n",
    "inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKzbJKfOgGV8"
   },
   "source": [
    "2. Ініціюйте ваги `w`, `b` для моделі логістичної регресії потрібної форми зважаючи на розмірності даних випадковими значеннями з нормального розподілу. Лишаю тут код для фіксації `random_seed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "id": "aXhKw6Tdj1-d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1637b8df0>"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "id": "eApcB7eb6h9o"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.6614, 0.2669, 0.0617]], requires_grad=True),\n",
       " tensor([0.6213], requires_grad=True))"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.randn(1, 3, dtype=torch.float32, requires_grad=True)  # 1 вихід -  прогноз одного числа для яблук, 3 ознаки - temp, rainfall, humidity\n",
    "b = torch.randn(1, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYGxNGTaf5s6"
   },
   "source": [
    "3. Напишіть функцію `model`, яка буде обчислювати функцію гіпотези в логістичній регресії і дозволяти робити передбачення на основі введеного рядка даних і коефіцієнтів в змінних `w`, `b`.\n",
    "\n",
    "  **Важливий момент**, що функція `model` робить обчислення на `torch.tensors`, тож для математичних обчислень використовуємо фукнціонал `torch`, наприклад:\n",
    "  - обчсилення $e^x$: `torch.exp(x)`\n",
    "  - обчсилення $log(x)$: `torch.log(x)`\n",
    "  - обчислення середнього значення вектору `x`: `torch.mean(x)`\n",
    "\n",
    "  Використайте функцію `model` для обчислення передбачень з поточними значеннями `w`, `b`.Виведіть результат обчислень на екран.\n",
    "\n",
    "  Проаналізуйте передбачення. Чи не викликають вони у вас підозр? І якщо викликають, то чим це може бути зумовлено?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "id": "pSz2j4Fh6jBv"
   },
   "outputs": [],
   "source": [
    "# Функція для обчислення гіпотези в логістичній регресії\n",
    "def model(x, w, b):\n",
    "    linear_output = x @ w.t() + b\n",
    "    predictions = 1 / (1 + torch.exp(-linear_output))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model(inputs, w, b)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас у всіх випадках передбачає більше за 80 яблук врожайності, що, звісно, виглядає упереджено."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O2AGM0Mb2yHa"
   },
   "source": [
    "4. Напишіть функцію `binary_cross_entropy`, яка приймає на вхід передбачення моделі `predicted_probs` та справжні мітки в даних `true_labels` і обчислює значення втрат (loss)  за формулою бінарної крос-ентропії для кожного екземпляра та вертає середні втрати по всьому набору даних.\n",
    "  Використайте функцію `binary_cross_entropy` для обчислення втрат для поточних передбачень моделі."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "id": "1bWlovvx6kZS"
   },
   "outputs": [],
   "source": [
    "# def binary_cross_entropy(predicted_probs, true_labels):\n",
    "#     epsilon = 1e-7     # Запобігаємо логарифмуванню дуже малих значень (додаємо epsilon)\n",
    "#     predicted_probs = torch.clamp(predicted_probs, min=epsilon, max=1-epsilon)\n",
    "#     bce_loss = - (true_labels * torch.log(predicted_probs) + (1 - true_labels) * torch.log(1 - predicted_probs))\n",
    "#     return torch.mean(bce_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(predicted_probs, true_labels):\n",
    "    bce_loss = - (true_labels * torch.log(predicted_probs) + (1 - true_labels) * torch.log(1 - predicted_probs))\n",
    "    return torch.mean(bce_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = binary_cross_entropy(prediction, targets)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFKpQxdHi1__"
   },
   "source": [
    "5. Зробіть зворотнє поширення помилки і виведіть градієнти за параметрами `w`, `b`. Проаналізуйте їх значення. Як гадаєте, чому вони саме такі?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "id": "YAbXUNSJ6mCl"
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.6614, 0.2669, 0.0617]], requires_grad=True),\n",
       " tensor([[nan, nan, nan]]))"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.6213], requires_grad=True), tensor([nan]))"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b, b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "отримали nan. Значення передбачень були занадто близькими до 1, що спричинило проблему під час обчислення логарифмів"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDN1t1RujQsK"
   },
   "source": [
    "**Що сталось?**\n",
    "\n",
    "В цій задачі, коли ми ініціювали значення випадковими значеннями з нормального розподілу, насправді ці значення не були дуже гарними стартовими значеннями і привели до того, що градієнти стали дуже малими або навіть рівними нулю (це призводить до того, що градієнти \"зникають\"), і відповідно при оновленні ваг у нас не буде нічого змінюватись. Це називається `gradient vanishing`. Це відбувається через **насичення сигмоїдної функції активації.**\n",
    "\n",
    "У нашій задачі ми використовуємо сигмоїдну функцію активації, яка має такий вигляд:\n",
    "\n",
    "   $$\n",
    "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "   $$\n",
    "\n",
    "\n",
    "Коли значення $z$ дуже велике або дуже мале, сигмоїдна функція починає \"насичуватись\". Це означає, що для великих позитивних $z$ сигмоїда наближається до 1, а для великих негативних — до 0. В цих діапазонах градієнти починають стрімко зменшуватись і наближаються до нуля (бо градієнт - це похідна, похідна на проміжку функції, де вона паралельна осі ОХ, дорівнює 0), що робить оновлення ваг неможливим.\n",
    "\n",
    "![](https://editor.analyticsvidhya.com/uploads/27889vaegp.png)\n",
    "\n",
    "У логістичній регресії $ z = x \\cdot w + b $. Якщо ваги $w, b$ - великі, значення $z$ також буде великим, і сигмоїда перейде в насичену область, де градієнти дуже малі.\n",
    "\n",
    "Саме це сталося в нашій задачі, де великі випадкові значення ваг викликали насичення сигмоїдної функції. Це в свою чергу призводить до того, що під час зворотного поширення помилки (backpropagation) модель оновлює ваги дуже повільно або зовсім не оновлює. Це називається проблемою **зникнення градієнтів** (gradient vanishing problem).\n",
    "\n",
    "**Що ж робити?**\n",
    "Ініціювати ваги маленькими значеннями навколо нуля. Наприклад ми можемо просто в існуючій ініціалізації ваги розділити на 1000. Можна також використати інший спосіб ініціалізації вагів - інформація про це [тут](https://www.geeksforgeeks.org/initialize-weights-in-pytorch/).\n",
    "\n",
    "Як це робити - показую нижче. **Виконайте код та знову обчисліть передбачення, лосс і виведіть градієнти.**\n",
    "\n",
    "А я пишу пояснення, чому просто не зробити\n",
    "\n",
    "```\n",
    "w = torch.randn(1, 3, requires_grad=True)/1000\n",
    "b = torch.randn(1, requires_grad=True)/1000\n",
    "```\n",
    "\n",
    "Нам потрібно, аби тензори вагів були листовими (leaf tensors).\n",
    "\n",
    "1. **Що таке листовий тензор**\n",
    "Листовий тензор — це тензор, який був створений користувачем безпосередньо і з якого починається обчислювальний граф. Якщо такий тензор має `requires_grad=True`, PyTorch буде відслідковувати всі операції, виконані над ним, щоб правильно обчислювати градієнти під час навчання.\n",
    "\n",
    "2. **Чому ми використовуємо `w.data` замість звичайних операцій**\n",
    "Якщо ми просто виконали б операції, такі як `(w - 0.5) / 100`, ми б отримали **новий тензор**, який вже не був би листовим тензором, оскільки ці операції створюють **новий** тензор, а не модифікують існуючий.\n",
    "\n",
    "  Проте, щоб залишити наші тензори ваги `w` та зміщення `b` листовими і продовжити можливість відстеження градієнтів під час тренування, ми використовуємо атрибут `.data`. Цей атрибут дозволяє **виконувати операції in-place (прямо на існуючому тензорі)** без зміни самого об'єкта тензора. Отже, тензор залишається листовим, і PyTorch може коректно обчислювати його градієнти.\n",
    "\n",
    "3. **Чому важливо залишити тензор листовим**\n",
    "Якщо тензор більше не є листовим (наприклад, через проведення операцій, що створюють нові тензори), ви не зможете отримати градієнти за допомогою `w.grad` чи `b.grad` після виклику `loss.backward()`. Це може призвести до втрати можливості оновлення параметрів під час тренування моделі. В нашому випадку ми хочемо, щоб тензори `w` та `b` накопичували градієнти, тому вони повинні залишатись листовими.\n",
    "\n",
    "**Висновок:**\n",
    "Ми використовуємо `.data`, щоб виконати операції зміни значень на ваги і зміщення **in-place**, залишаючи їх листовими тензорами, які можуть накопичувати градієнти під час навчання. Це дозволяє коректно працювати механізму зворотного поширення помилки (backpropagation) і оновлювати ваги моделі."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOPSQyttpVjO"
   },
   "source": [
    "5. Виконайте код та знову обчисліть передбачення, лосс і знайдіть градієнти та виведіть всі ці тензори на екран."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "id": "-EBOJ3tsnRaD"
   },
   "outputs": [],
   "source": [
    "torch.random.manual_seed(1)\n",
    "w = torch.randn(1, 3, requires_grad=True)  # Листовий тензор\n",
    "b = torch.randn(1, requires_grad=True)     # Листовий тензор\n",
    "\n",
    "# in-place операції\n",
    "w.data = w.data / 1000\n",
    "b.data = b.data / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "id": "-JwXiSpX6orh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5174],\n",
       "        [0.5220],\n",
       "        [0.5244],\n",
       "        [0.5204],\n",
       "        [0.5190]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model(inputs, w, b)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6829, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = binary_cross_entropy(prediction, targets)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[6.6135e-04, 2.6692e-04, 6.1677e-05]], requires_grad=True),\n",
       " tensor([[ -5.4417, -18.9853, -10.0682]]))"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0006], requires_grad=True), tensor([-0.0794]))"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b, b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCdi44IT334o"
   },
   "source": [
    "6. Напишіть алгоритм градієнтного спуску, який буде навчати модель з використанням написаних раніше функцій і виконуючи оновлення ваг. Алгоритм має включати наступні кроки:\n",
    "\n",
    "  1. Генерація прогнозів\n",
    "  2. Обчислення втрат\n",
    "  3. Обчислення градієнтів (gradients) loss-фукнції відносно ваг і зсувів\n",
    "  4. Налаштування ваг шляхом віднімання невеликої величини, пропорційної градієнту (`learning_rate` домножений на градієнт)\n",
    "  5. Скидання градієнтів на нуль\n",
    "\n",
    "Виконайте градієнтний спуск протягом 1000 епох, обчисліть фінальні передбачення і проаналізуйте, чи вони точні?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "id": "mObHPyE06qsO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000, Loss: 0.5625792145729065\n",
      "Epoch 200/1000, Loss: 0.5070590972900391\n",
      "Epoch 300/1000, Loss: 0.4650115370750427\n",
      "Epoch 400/1000, Loss: 0.4326744079589844\n",
      "Epoch 500/1000, Loss: 0.40733838081359863\n",
      "Epoch 600/1000, Loss: 0.3871012330055237\n",
      "Epoch 700/1000, Loss: 0.37063437700271606\n",
      "Epoch 800/1000, Loss: 0.3570035398006439\n",
      "Epoch 900/1000, Loss: 0.3455430269241333\n",
      "Epoch 1000/1000, Loss: 0.3357715606689453\n",
      "Фінальні передбачення:\n",
      " tensor([[0.5777],\n",
      "        [0.6685],\n",
      "        [0.9113],\n",
      "        [0.1616],\n",
      "        [0.8653]], grad_fn=<MulBackward0>)\n",
      "Фінальні бінарні передбачення:\n",
      " tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]])\n",
      "Точність моделі: 80.00%\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-5\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    predictions = model(inputs, w, b)\n",
    "\n",
    "    loss = binary_cross_entropy(predictions, targets)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "final_predictions = model(inputs, w, b)\n",
    "print(\"Фінальні передбачення:\\n\", final_predictions)\n",
    "\n",
    "final_binary_predictions = (final_predictions >= 0.5).float()\n",
    "print(\"Фінальні бінарні передбачення:\\n\", final_binary_predictions)\n",
    "\n",
    "accuracy = (final_binary_predictions == targets).float().mean()\n",
    "print(f\"Точність моделі: {accuracy.item() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вийшла доволі непогана модель з точністю 80%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuRhlyF9qAia"
   },
   "source": [
    "**Секція 2. Створення лог регресії з використанням функціоналу `torch.nn`.**\n",
    "\n",
    "Давайте повторно реалізуємо ту ж модель, використовуючи деякі вбудовані функції та класи з PyTorch.\n",
    "\n",
    "Даних у нас буде побільше - тож, визначаємо нові масиви."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "id": "IX8Bhm74rV4M"
   },
   "outputs": [],
   "source": [
    "# Вхідні дані (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43],\n",
    "                   [91, 88, 64],\n",
    "                   [87, 134, 58],\n",
    "                   [102, 43, 37],\n",
    "                   [69, 96, 70],\n",
    "                   [73, 67, 43],\n",
    "                   [91, 88, 64],\n",
    "                   [87, 134, 58],\n",
    "                   [102, 43, 37],\n",
    "                   [69, 96, 70],\n",
    "                   [73, 67, 43],\n",
    "                   [91, 88, 64],\n",
    "                   [87, 134, 58],\n",
    "                   [102, 43, 37],\n",
    "                   [69, 96, 70]], dtype='float32')\n",
    "\n",
    "# Таргети (apples > 80)\n",
    "targets = np.array([[0],\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [1]], dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7X2dV30KtAPu"
   },
   "source": [
    "7. Завантажте вхідні дані та мітки в PyTorch тензори та з них створіть датасет, який поєднує вхідні дані з мітками, використовуючи клас `TensorDataset`. Виведіть перші 3 елементи в датасеті.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "id": "chrvMfBs6vjo"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "targets = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "dataset = TensorDataset(inputs_tensor, targets_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 73.,  67.,  43.],\n",
       "         [ 91.,  88.,  64.],\n",
       "         [ 87., 134.,  58.]]),\n",
       " tensor([[0.],\n",
       "         [1.],\n",
       "         [1.]]))"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nMFaa8suOd3"
   },
   "source": [
    "8. Визначте data loader з класом **DataLoader** для підготовленого датасету `train_ds`, встановіть розмір батчу на 5 та увімкніть перемішування даних для ефективного навчання моделі. Виведіть перший елемент в дата лоадері."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "id": "ZCsRo5Mx6wEI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([73., 67., 43.]), tensor([0.]))"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=5, shuffle=True)\n",
    "\n",
    "first_batch = next(iter(train_loader))\n",
    "\n",
    "first_batch[0][0], first_batch[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymcQOo_hum6I"
   },
   "source": [
    "9. Створіть клас `LogReg` для логістичної регресії, наслідуючи модуль `torch.nn.Module` за прикладом в лекції (в частині про FeedForward мережі).\n",
    "\n",
    "  У нас модель складається з лінійної комбінації вхідних значень і застосування фукнції сигмоїда. Тож, нейромережа буде складатись з лінійного шару `nn.Linear` і використання активації `nn.Sigmid`. У створеному класі мають бути реалізовані методи `__init__` з ініціалізацією шарів і метод `forward` для виконання прямого проходу моделі через лінійний шар і функцію активації.\n",
    "\n",
    "  Створіть екземпляр класу `LogReg` в змінній `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "id": "EyAwhTBW6xxz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogReg(\n",
       "  (linear): Linear(in_features=3, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LogReg(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        \"\"\"\n",
    "        Ініціалізація класу логістичної регресії.\n",
    "        \n",
    "        :param input_size: Кількість ознак (features) на вхід\n",
    "        :param output_size: Кількість виходів (output), зазвичай 1 для бінарної класифікації\n",
    "        \"\"\"\n",
    "        super(LogReg, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Прямий прохід через модель.\n",
    "        \n",
    "        :param x: Вхідні дані (тензор)\n",
    "        :return: Результати моделі (передбачення)\n",
    "        \"\"\"\n",
    "        linear_output = self.linear(x)\n",
    "        return self.sigmoid(linear_output)\n",
    "\n",
    "input_size = 3  \n",
    "output_size = 1\n",
    "\n",
    "model = LogReg(input_size, output_size)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RflV7xeVyoJy"
   },
   "source": [
    "10. Задайте оптимізатор `Stockastic Gradient Descent` в змінній `opt` для навчання моделі логістичної регресії. А також визначіть в змінній `loss` функцію втрат `binary_cross_entropy` з модуля `torch.nn.functional` для обчислення втрат моделі. Обчисліть втрати для поточних передбачень і міток, а потім виведіть їх. Зробіть висновок, чи моделі вдалось навчитись?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "id": "3QCATPU_6yfa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.6312, grad_fn=<BinaryCrossEntropyBackward0>)"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Параметри\n",
    "learning_rate = 1e-5\n",
    "epochs = 1000\n",
    "batch_size = 5\n",
    "\n",
    "# Створення оптимізатора Stockastic Gradient Descent (SGD)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Функція втрат\n",
    "loss_fn = F.binary_cross_entropy\n",
    "\n",
    "loss = loss_fn(model(inputs), targets)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ch-WrYnKzMzq"
   },
   "source": [
    "11. Візьміть з лекції функцію для тренування моделі з відстеженням значень втрат і навчіть щойно визначену модель на 1000 епохах. Виведіть після цього графік зміни loss, фінальні передбачення і значення таргетів."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 7.590260028839111\n",
      "Epoch [2/1000], Loss: 7.466097990671794\n",
      "Epoch [3/1000], Loss: 7.347989400227864\n",
      "Epoch [4/1000], Loss: 7.238023122151692\n",
      "Epoch [5/1000], Loss: 7.1360955238342285\n",
      "Epoch [6/1000], Loss: 7.012691020965576\n",
      "Epoch [7/1000], Loss: 6.917130470275879\n",
      "Epoch [8/1000], Loss: 6.817821979522705\n",
      "Epoch [9/1000], Loss: 6.729780991872151\n",
      "Epoch [10/1000], Loss: 6.640044689178467\n",
      "Epoch [11/1000], Loss: 6.563978354136149\n",
      "Epoch [12/1000], Loss: 6.491995493570964\n",
      "Epoch [13/1000], Loss: 6.443535486857097\n",
      "Epoch [14/1000], Loss: 6.355709075927734\n",
      "Epoch [15/1000], Loss: 6.294747670491536\n",
      "Epoch [16/1000], Loss: 6.2434327602386475\n",
      "Epoch [17/1000], Loss: 6.195674498875936\n",
      "Epoch [18/1000], Loss: 6.143290996551514\n",
      "Epoch [19/1000], Loss: 6.097677071889241\n",
      "Epoch [20/1000], Loss: 6.048301617304484\n",
      "Epoch [21/1000], Loss: 6.012735366821289\n",
      "Epoch [22/1000], Loss: 5.973509788513184\n",
      "Epoch [23/1000], Loss: 5.935581207275391\n",
      "Epoch [24/1000], Loss: 5.905618826548259\n",
      "Epoch [25/1000], Loss: 5.876819133758545\n",
      "Epoch [26/1000], Loss: 5.860766092936198\n",
      "Epoch [27/1000], Loss: 5.8340457280476885\n",
      "Epoch [28/1000], Loss: 5.786581675211589\n",
      "Epoch [29/1000], Loss: 5.751483122507731\n",
      "Epoch [30/1000], Loss: 5.726339817047119\n",
      "Epoch [31/1000], Loss: 5.686006148656209\n",
      "Epoch [32/1000], Loss: 5.673023700714111\n",
      "Epoch [33/1000], Loss: 5.64771048227946\n",
      "Epoch [34/1000], Loss: 5.628239711125691\n",
      "Epoch [35/1000], Loss: 5.597224712371826\n",
      "Epoch [36/1000], Loss: 5.576443990071614\n",
      "Epoch [37/1000], Loss: 5.537994305292766\n",
      "Epoch [38/1000], Loss: 5.525928338368733\n",
      "Epoch [39/1000], Loss: 5.511444727579753\n",
      "Epoch [40/1000], Loss: 5.470961332321167\n",
      "Epoch [41/1000], Loss: 5.453318119049072\n",
      "Epoch [42/1000], Loss: 5.4381288687388105\n",
      "Epoch [43/1000], Loss: 5.4195663134257\n",
      "Epoch [44/1000], Loss: 5.38118314743042\n",
      "Epoch [45/1000], Loss: 5.378613154093425\n",
      "Epoch [46/1000], Loss: 5.332521915435791\n",
      "Epoch [47/1000], Loss: 5.315375328063965\n",
      "Epoch [48/1000], Loss: 5.298693497975667\n",
      "Epoch [49/1000], Loss: 5.28675635655721\n",
      "Epoch [50/1000], Loss: 5.247402747472127\n",
      "Epoch [51/1000], Loss: 5.224597771962483\n",
      "Epoch [52/1000], Loss: 5.204712629318237\n",
      "Epoch [53/1000], Loss: 5.218013207117717\n",
      "Epoch [54/1000], Loss: 5.176572481791179\n",
      "Epoch [55/1000], Loss: 5.155567010243733\n",
      "Epoch [56/1000], Loss: 5.128400405248006\n",
      "Epoch [57/1000], Loss: 5.114836851755778\n",
      "Epoch [58/1000], Loss: 5.111258188883464\n",
      "Epoch [59/1000], Loss: 5.064071973164876\n",
      "Epoch [60/1000], Loss: 5.065328598022461\n",
      "Epoch [61/1000], Loss: 5.052112499872844\n",
      "Epoch [62/1000], Loss: 5.04292631149292\n",
      "Epoch [63/1000], Loss: 5.019121249516805\n",
      "Epoch [64/1000], Loss: 5.000293095906575\n",
      "Epoch [65/1000], Loss: 4.977041085561116\n",
      "Epoch [66/1000], Loss: 4.953528165817261\n",
      "Epoch [67/1000], Loss: 4.946953376134236\n",
      "Epoch [68/1000], Loss: 4.9192713896433515\n",
      "Epoch [69/1000], Loss: 4.888109525044759\n",
      "Epoch [70/1000], Loss: 4.897421558698018\n",
      "Epoch [71/1000], Loss: 4.863364934921265\n",
      "Epoch [72/1000], Loss: 4.839607238769531\n",
      "Epoch [73/1000], Loss: 4.823021411895752\n",
      "Epoch [74/1000], Loss: 4.8468888600667315\n",
      "Epoch [75/1000], Loss: 4.845129410425822\n",
      "Epoch [76/1000], Loss: 4.806654453277588\n",
      "Epoch [77/1000], Loss: 4.829858700434367\n",
      "Epoch [78/1000], Loss: 4.828983505566915\n",
      "Epoch [79/1000], Loss: 4.797117392222087\n",
      "Epoch [80/1000], Loss: 4.786130905151367\n",
      "Epoch [81/1000], Loss: 4.763120969136556\n",
      "Epoch [82/1000], Loss: 4.755742073059082\n",
      "Epoch [83/1000], Loss: 4.736754020055135\n",
      "Epoch [84/1000], Loss: 4.725619713465373\n",
      "Epoch [85/1000], Loss: 4.707547028859456\n",
      "Epoch [86/1000], Loss: 4.7079207102457685\n",
      "Epoch [87/1000], Loss: 4.703450520833333\n",
      "Epoch [88/1000], Loss: 4.683403491973877\n",
      "Epoch [89/1000], Loss: 4.669968287150065\n",
      "Epoch [90/1000], Loss: 4.69032879670461\n",
      "Epoch [91/1000], Loss: 4.660061836242676\n",
      "Epoch [92/1000], Loss: 4.6539937655131025\n",
      "Epoch [93/1000], Loss: 4.627196788787842\n",
      "Epoch [94/1000], Loss: 4.622295220692952\n",
      "Epoch [95/1000], Loss: 4.601389726003011\n",
      "Epoch [96/1000], Loss: 4.5717423756917315\n",
      "Epoch [97/1000], Loss: 4.583244323730469\n",
      "Epoch [98/1000], Loss: 4.55248498916626\n",
      "Epoch [99/1000], Loss: 4.540371735890706\n",
      "Epoch [100/1000], Loss: 4.5352745453516645\n",
      "Epoch [101/1000], Loss: 4.547775109608968\n",
      "Epoch [102/1000], Loss: 4.511369705200195\n",
      "Epoch [103/1000], Loss: 4.510378360748291\n",
      "Epoch [104/1000], Loss: 4.506761948267619\n",
      "Epoch [105/1000], Loss: 4.497812271118164\n",
      "Epoch [106/1000], Loss: 4.497061252593994\n",
      "Epoch [107/1000], Loss: 4.490736087163289\n",
      "Epoch [108/1000], Loss: 4.488037745157878\n",
      "Epoch [109/1000], Loss: 4.495280424753825\n",
      "Epoch [110/1000], Loss: 4.475324908892314\n",
      "Epoch [111/1000], Loss: 4.46104097366333\n",
      "Epoch [112/1000], Loss: 4.435400009155273\n",
      "Epoch [113/1000], Loss: 4.415453910827637\n",
      "Epoch [114/1000], Loss: 4.443251212437947\n",
      "Epoch [115/1000], Loss: 4.404659748077393\n",
      "Epoch [116/1000], Loss: 4.411558787027995\n",
      "Epoch [117/1000], Loss: 4.393288691838582\n",
      "Epoch [118/1000], Loss: 4.3906034628550215\n",
      "Epoch [119/1000], Loss: 4.390684207280477\n",
      "Epoch [120/1000], Loss: 4.368618567784627\n",
      "Epoch [121/1000], Loss: 4.383085489273071\n",
      "Epoch [122/1000], Loss: 4.3504728476206465\n",
      "Epoch [123/1000], Loss: 4.3591636419296265\n",
      "Epoch [124/1000], Loss: 4.352633873621623\n",
      "Epoch [125/1000], Loss: 4.327412048975627\n",
      "Epoch [126/1000], Loss: 4.328839619954427\n",
      "Epoch [127/1000], Loss: 4.327127238114675\n",
      "Epoch [128/1000], Loss: 4.3136405150095625\n",
      "Epoch [129/1000], Loss: 4.289002100626628\n",
      "Epoch [130/1000], Loss: 4.279236634572347\n",
      "Epoch [131/1000], Loss: 4.293225248654683\n",
      "Epoch [132/1000], Loss: 4.272662242253621\n",
      "Epoch [133/1000], Loss: 4.269398490587871\n",
      "Epoch [134/1000], Loss: 4.249099612236023\n",
      "Epoch [135/1000], Loss: 4.2421770095825195\n",
      "Epoch [136/1000], Loss: 4.239013115564982\n",
      "Epoch [137/1000], Loss: 4.238180120786031\n",
      "Epoch [138/1000], Loss: 4.218091607093811\n",
      "Epoch [139/1000], Loss: 4.218657871087392\n",
      "Epoch [140/1000], Loss: 4.201780478159587\n",
      "Epoch [141/1000], Loss: 4.197284817695618\n",
      "Epoch [142/1000], Loss: 4.173321803410848\n",
      "Epoch [143/1000], Loss: 4.172769228617351\n",
      "Epoch [144/1000], Loss: 4.166673580805461\n",
      "Epoch [145/1000], Loss: 4.161318560441335\n",
      "Epoch [146/1000], Loss: 4.160277684529622\n",
      "Epoch [147/1000], Loss: 4.138287941614787\n",
      "Epoch [148/1000], Loss: 4.131005922953288\n",
      "Epoch [149/1000], Loss: 4.119288047154744\n",
      "Epoch [150/1000], Loss: 4.11316704750061\n",
      "Epoch [151/1000], Loss: 4.111240923404694\n",
      "Epoch [152/1000], Loss: 4.102568864822388\n",
      "Epoch [153/1000], Loss: 4.092051347096761\n",
      "Epoch [154/1000], Loss: 4.082539876302083\n",
      "Epoch [155/1000], Loss: 4.071869691212972\n",
      "Epoch [156/1000], Loss: 4.062093059221904\n",
      "Epoch [157/1000], Loss: 4.063252528508504\n",
      "Epoch [158/1000], Loss: 4.054614226023356\n",
      "Epoch [159/1000], Loss: 4.0428478717803955\n",
      "Epoch [160/1000], Loss: 4.0389058192571\n",
      "Epoch [161/1000], Loss: 4.031976699829102\n",
      "Epoch [162/1000], Loss: 4.015316088994344\n",
      "Epoch [163/1000], Loss: 4.003673871358235\n",
      "Epoch [164/1000], Loss: 3.9976174036661782\n",
      "Epoch [165/1000], Loss: 3.994140148162842\n",
      "Epoch [166/1000], Loss: 3.9820359349250793\n",
      "Epoch [167/1000], Loss: 3.9704421361287436\n",
      "Epoch [168/1000], Loss: 3.9582406679789224\n",
      "Epoch [169/1000], Loss: 3.9579058488210044\n",
      "Epoch [170/1000], Loss: 3.942289193471273\n",
      "Epoch [171/1000], Loss: 3.935668150583903\n",
      "Epoch [172/1000], Loss: 3.9376025199890137\n",
      "Epoch [173/1000], Loss: 3.9352422753969827\n",
      "Epoch [174/1000], Loss: 3.925015926361084\n",
      "Epoch [175/1000], Loss: 3.9091901779174805\n",
      "Epoch [176/1000], Loss: 3.8946701685587564\n",
      "Epoch [177/1000], Loss: 3.8906936645507812\n",
      "Epoch [178/1000], Loss: 3.8780574798583984\n",
      "Epoch [179/1000], Loss: 3.8843262990315757\n",
      "Epoch [180/1000], Loss: 3.8700940211613974\n",
      "Epoch [181/1000], Loss: 3.8491751750310264\n",
      "Epoch [182/1000], Loss: 3.843429962793986\n",
      "Epoch [183/1000], Loss: 3.847675323486328\n",
      "Epoch [184/1000], Loss: 3.82992160320282\n",
      "Epoch [185/1000], Loss: 3.8273401260375977\n",
      "Epoch [186/1000], Loss: 3.8158915440241494\n",
      "Epoch [187/1000], Loss: 3.8066789309183755\n",
      "Epoch [188/1000], Loss: 3.7978667418162027\n",
      "Epoch [189/1000], Loss: 3.7890755335489907\n",
      "Epoch [190/1000], Loss: 3.7755850156148276\n",
      "Epoch [191/1000], Loss: 3.7674594720204673\n",
      "Epoch [192/1000], Loss: 3.756699721018473\n",
      "Epoch [193/1000], Loss: 3.764276842276255\n",
      "Epoch [194/1000], Loss: 3.741643746693929\n",
      "Epoch [195/1000], Loss: 3.7399755318959556\n",
      "Epoch [196/1000], Loss: 3.737895687421163\n",
      "Epoch [197/1000], Loss: 3.732063412666321\n",
      "Epoch [198/1000], Loss: 3.712953726450602\n",
      "Epoch [199/1000], Loss: 3.701063593228658\n",
      "Epoch [200/1000], Loss: 3.6974473794301352\n",
      "Epoch [201/1000], Loss: 3.6826605796813965\n",
      "Epoch [202/1000], Loss: 3.684814214706421\n",
      "Epoch [203/1000], Loss: 3.6673396825790405\n",
      "Epoch [204/1000], Loss: 3.662583112716675\n",
      "Epoch [205/1000], Loss: 3.6510072549184165\n",
      "Epoch [206/1000], Loss: 3.6452573935190835\n",
      "Epoch [207/1000], Loss: 3.6370418071746826\n",
      "Epoch [208/1000], Loss: 3.62534228960673\n",
      "Epoch [209/1000], Loss: 3.6189366976420083\n",
      "Epoch [210/1000], Loss: 3.610073685646057\n",
      "Epoch [211/1000], Loss: 3.602827787399292\n",
      "Epoch [212/1000], Loss: 3.596351385116577\n",
      "Epoch [213/1000], Loss: 3.595167954762777\n",
      "Epoch [214/1000], Loss: 3.590299745400747\n",
      "Epoch [215/1000], Loss: 3.5689765214920044\n",
      "Epoch [216/1000], Loss: 3.558264176050822\n",
      "Epoch [217/1000], Loss: 3.5548311869303384\n",
      "Epoch [218/1000], Loss: 3.5435706774393716\n",
      "Epoch [219/1000], Loss: 3.5454825361569724\n",
      "Epoch [220/1000], Loss: 3.529725193977356\n",
      "Epoch [221/1000], Loss: 3.52172581354777\n",
      "Epoch [222/1000], Loss: 3.5093890031178794\n",
      "Epoch [223/1000], Loss: 3.4984240531921387\n",
      "Epoch [224/1000], Loss: 3.493666887283325\n",
      "Epoch [225/1000], Loss: 3.4856053988138833\n",
      "Epoch [226/1000], Loss: 3.4816534519195557\n",
      "Epoch [227/1000], Loss: 3.4664272467295327\n",
      "Epoch [228/1000], Loss: 3.4614851077397666\n",
      "Epoch [229/1000], Loss: 3.4486936728159585\n",
      "Epoch [230/1000], Loss: 3.4427194595336914\n",
      "Epoch [231/1000], Loss: 3.432804822921753\n",
      "Epoch [232/1000], Loss: 3.4283603032430015\n",
      "Epoch [233/1000], Loss: 3.428455869356791\n",
      "Epoch [234/1000], Loss: 3.411460796991984\n",
      "Epoch [235/1000], Loss: 3.409175395965576\n",
      "Epoch [236/1000], Loss: 3.3944234053293862\n",
      "Epoch [237/1000], Loss: 3.393242279688517\n",
      "Epoch [238/1000], Loss: 3.398835778236389\n",
      "Epoch [239/1000], Loss: 3.3663652737935386\n",
      "Epoch [240/1000], Loss: 3.3759883840878806\n",
      "Epoch [241/1000], Loss: 3.353297313054403\n",
      "Epoch [242/1000], Loss: 3.347367604573568\n",
      "Epoch [243/1000], Loss: 3.336268345514933\n",
      "Epoch [244/1000], Loss: 3.3277928829193115\n",
      "Epoch [245/1000], Loss: 3.317108154296875\n",
      "Epoch [246/1000], Loss: 3.314124584197998\n",
      "Epoch [247/1000], Loss: 3.307213226954142\n",
      "Epoch [248/1000], Loss: 3.2937300205230713\n",
      "Epoch [249/1000], Loss: 3.2896440029144287\n",
      "Epoch [250/1000], Loss: 3.2822250525156655\n",
      "Epoch [251/1000], Loss: 3.271265983581543\n",
      "Epoch [252/1000], Loss: 3.2647108236948648\n",
      "Epoch [253/1000], Loss: 3.2550792694091797\n",
      "Epoch [254/1000], Loss: 3.2496354977289834\n",
      "Epoch [255/1000], Loss: 3.2399470011393228\n",
      "Epoch [256/1000], Loss: 3.22716490427653\n",
      "Epoch [257/1000], Loss: 3.22320020198822\n",
      "Epoch [258/1000], Loss: 3.226020336151123\n",
      "Epoch [259/1000], Loss: 3.204933285713196\n",
      "Epoch [260/1000], Loss: 3.198459585507711\n",
      "Epoch [261/1000], Loss: 3.187692483266195\n",
      "Epoch [262/1000], Loss: 3.1814281940460205\n",
      "Epoch [263/1000], Loss: 3.1777328650156655\n",
      "Epoch [264/1000], Loss: 3.1633192698160806\n",
      "Epoch [265/1000], Loss: 3.155385732650757\n",
      "Epoch [266/1000], Loss: 3.1585635344187417\n",
      "Epoch [267/1000], Loss: 3.1413002014160156\n",
      "Epoch [268/1000], Loss: 3.1305359601974487\n",
      "Epoch [269/1000], Loss: 3.1251514156659446\n",
      "Epoch [270/1000], Loss: 3.1111178398132324\n",
      "Epoch [271/1000], Loss: 3.1045349836349487\n",
      "Epoch [272/1000], Loss: 3.0948002338409424\n",
      "Epoch [273/1000], Loss: 3.0951039791107178\n",
      "Epoch [274/1000], Loss: 3.0907434622446694\n",
      "Epoch [275/1000], Loss: 3.078639586766561\n",
      "Epoch [276/1000], Loss: 3.071253776550293\n",
      "Epoch [277/1000], Loss: 3.055203914642334\n",
      "Epoch [278/1000], Loss: 3.0567496617635093\n",
      "Epoch [279/1000], Loss: 3.039647897084554\n",
      "Epoch [280/1000], Loss: 3.0307729244232178\n",
      "Epoch [281/1000], Loss: 3.023859182993571\n",
      "Epoch [282/1000], Loss: 3.0155949195226035\n",
      "Epoch [283/1000], Loss: 3.0058434009552\n",
      "Epoch [284/1000], Loss: 3.0005210240681968\n",
      "Epoch [285/1000], Loss: 3.000598589579264\n",
      "Epoch [286/1000], Loss: 2.980200926462809\n",
      "Epoch [287/1000], Loss: 2.984272758165995\n",
      "Epoch [288/1000], Loss: 2.970230976740519\n",
      "Epoch [289/1000], Loss: 2.960399309794108\n",
      "Epoch [290/1000], Loss: 2.9483444690704346\n",
      "Epoch [291/1000], Loss: 2.94462780157725\n",
      "Epoch [292/1000], Loss: 2.9347676038742065\n",
      "Epoch [293/1000], Loss: 2.924384673436483\n",
      "Epoch [294/1000], Loss: 2.914396365483602\n",
      "Epoch [295/1000], Loss: 2.9229214191436768\n",
      "Epoch [296/1000], Loss: 2.9040494759877524\n",
      "Epoch [297/1000], Loss: 2.89006245136261\n",
      "Epoch [298/1000], Loss: 2.891465187072754\n",
      "Epoch [299/1000], Loss: 2.8863640228907266\n",
      "Epoch [300/1000], Loss: 2.87475057442983\n",
      "Epoch [301/1000], Loss: 2.860525608062744\n",
      "Epoch [302/1000], Loss: 2.858647028605143\n",
      "Epoch [303/1000], Loss: 2.844787915547689\n",
      "Epoch [304/1000], Loss: 2.8426905969778695\n",
      "Epoch [305/1000], Loss: 2.829875032107035\n",
      "Epoch [306/1000], Loss: 2.8287318547566733\n",
      "Epoch [307/1000], Loss: 2.8145593404769897\n",
      "Epoch [308/1000], Loss: 2.8046842416127524\n",
      "Epoch [309/1000], Loss: 2.798899014790853\n",
      "Epoch [310/1000], Loss: 2.7879464626312256\n",
      "Epoch [311/1000], Loss: 2.798859198888143\n",
      "Epoch [312/1000], Loss: 2.774832328160604\n",
      "Epoch [313/1000], Loss: 2.7667468388875327\n",
      "Epoch [314/1000], Loss: 2.7555490334828696\n",
      "Epoch [315/1000], Loss: 2.756532669067383\n",
      "Epoch [316/1000], Loss: 2.7394814491271973\n",
      "Epoch [317/1000], Loss: 2.728991746902466\n",
      "Epoch [318/1000], Loss: 2.7220353484153748\n",
      "Epoch [319/1000], Loss: 2.715897579987844\n",
      "Epoch [320/1000], Loss: 2.7048401832580566\n",
      "Epoch [321/1000], Loss: 2.6982266902923584\n",
      "Epoch [322/1000], Loss: 2.690140644709269\n",
      "Epoch [323/1000], Loss: 2.684645136197408\n",
      "Epoch [324/1000], Loss: 2.6898800134658813\n",
      "Epoch [325/1000], Loss: 2.664766232172648\n",
      "Epoch [326/1000], Loss: 2.661292870839437\n",
      "Epoch [327/1000], Loss: 2.6550029118855796\n",
      "Epoch [328/1000], Loss: 2.6517608165740967\n",
      "Epoch [329/1000], Loss: 2.634324828783671\n",
      "Epoch [330/1000], Loss: 2.629183769226074\n",
      "Epoch [331/1000], Loss: 2.6177805264790854\n",
      "Epoch [332/1000], Loss: 2.6246559222539267\n",
      "Epoch [333/1000], Loss: 2.6005876859029136\n",
      "Epoch [334/1000], Loss: 2.595585346221924\n",
      "Epoch [335/1000], Loss: 2.586025357246399\n",
      "Epoch [336/1000], Loss: 2.5815566380818686\n",
      "Epoch [337/1000], Loss: 2.572564204533895\n",
      "Epoch [338/1000], Loss: 2.578154762585958\n",
      "Epoch [339/1000], Loss: 2.5597058137257895\n",
      "Epoch [340/1000], Loss: 2.54561976591746\n",
      "Epoch [341/1000], Loss: 2.5427194237709045\n",
      "Epoch [342/1000], Loss: 2.53609299659729\n",
      "Epoch [343/1000], Loss: 2.529916524887085\n",
      "Epoch [344/1000], Loss: 2.5175612767537436\n",
      "Epoch [345/1000], Loss: 2.5121895472208657\n",
      "Epoch [346/1000], Loss: 2.50259268283844\n",
      "Epoch [347/1000], Loss: 2.4952803452809653\n",
      "Epoch [348/1000], Loss: 2.4849770069122314\n",
      "Epoch [349/1000], Loss: 2.495527466138204\n",
      "Epoch [350/1000], Loss: 2.470976988474528\n",
      "Epoch [351/1000], Loss: 2.4583425521850586\n",
      "Epoch [352/1000], Loss: 2.455926497777303\n",
      "Epoch [353/1000], Loss: 2.449278394381205\n",
      "Epoch [354/1000], Loss: 2.440382202466329\n",
      "Epoch [355/1000], Loss: 2.431450605392456\n",
      "Epoch [356/1000], Loss: 2.4253090620040894\n",
      "Epoch [357/1000], Loss: 2.412625710169474\n",
      "Epoch [358/1000], Loss: 2.4043416182200112\n",
      "Epoch [359/1000], Loss: 2.410396377245585\n",
      "Epoch [360/1000], Loss: 2.412203371524811\n",
      "Epoch [361/1000], Loss: 2.382737398147583\n",
      "Epoch [362/1000], Loss: 2.3838008443514505\n",
      "Epoch [363/1000], Loss: 2.3695046504338584\n",
      "Epoch [364/1000], Loss: 2.3577798207600913\n",
      "Epoch [365/1000], Loss: 2.349937081336975\n",
      "Epoch [366/1000], Loss: 2.3458863894144693\n",
      "Epoch [367/1000], Loss: 2.3458367586135864\n",
      "Epoch [368/1000], Loss: 2.3256585597991943\n",
      "Epoch [369/1000], Loss: 2.3212362130482993\n",
      "Epoch [370/1000], Loss: 2.3101600805918374\n",
      "Epoch [371/1000], Loss: 2.303644319375356\n",
      "Epoch [372/1000], Loss: 2.295717159907023\n",
      "Epoch [373/1000], Loss: 2.312700549761454\n",
      "Epoch [374/1000], Loss: 2.279034455617269\n",
      "Epoch [375/1000], Loss: 2.2726011276245117\n",
      "Epoch [376/1000], Loss: 2.279941121737162\n",
      "Epoch [377/1000], Loss: 2.2567973931630454\n",
      "Epoch [378/1000], Loss: 2.2482245763142905\n",
      "Epoch [379/1000], Loss: 2.2414311170578003\n",
      "Epoch [380/1000], Loss: 2.2362621625264487\n",
      "Epoch [381/1000], Loss: 2.2277070681254068\n",
      "Epoch [382/1000], Loss: 2.2239885330200195\n",
      "Epoch [383/1000], Loss: 2.2124356428782144\n",
      "Epoch [384/1000], Loss: 2.2025227546691895\n",
      "Epoch [385/1000], Loss: 2.1969233751296997\n",
      "Epoch [386/1000], Loss: 2.2017807165781655\n",
      "Epoch [387/1000], Loss: 2.1823856035868325\n",
      "Epoch [388/1000], Loss: 2.1816079020500183\n",
      "Epoch [389/1000], Loss: 2.1639194091161094\n",
      "Epoch [390/1000], Loss: 2.158653656641642\n",
      "Epoch [391/1000], Loss: 2.1546185413996377\n",
      "Epoch [392/1000], Loss: 2.142875591913859\n",
      "Epoch [393/1000], Loss: 2.1458237965901694\n",
      "Epoch [394/1000], Loss: 2.129827300707499\n",
      "Epoch [395/1000], Loss: 2.132600963115692\n",
      "Epoch [396/1000], Loss: 2.116155425707499\n",
      "Epoch [397/1000], Loss: 2.1042506297429404\n",
      "Epoch [398/1000], Loss: 2.0960124333699546\n",
      "Epoch [399/1000], Loss: 2.1018950939178467\n",
      "Epoch [400/1000], Loss: 2.0854082504908242\n",
      "Epoch [401/1000], Loss: 2.085356831550598\n",
      "Epoch [402/1000], Loss: 2.066726644833883\n",
      "Epoch [403/1000], Loss: 2.060719887415568\n",
      "Epoch [404/1000], Loss: 2.0509098768234253\n",
      "Epoch [405/1000], Loss: 2.0592883427937827\n",
      "Epoch [406/1000], Loss: 2.0374399026234946\n",
      "Epoch [407/1000], Loss: 2.033019185066223\n",
      "Epoch [408/1000], Loss: 2.0238470236460366\n",
      "Epoch [409/1000], Loss: 2.021505276362101\n",
      "Epoch [410/1000], Loss: 2.0061610539754233\n",
      "Epoch [411/1000], Loss: 1.9998161395390828\n",
      "Epoch [412/1000], Loss: 2.001278022925059\n",
      "Epoch [413/1000], Loss: 1.9866828123728435\n",
      "Epoch [414/1000], Loss: 1.98245902856191\n",
      "Epoch [415/1000], Loss: 1.975040316581726\n",
      "Epoch [416/1000], Loss: 1.96391894419988\n",
      "Epoch [417/1000], Loss: 1.959340512752533\n",
      "Epoch [418/1000], Loss: 1.9482376178105671\n",
      "Epoch [419/1000], Loss: 1.9407131671905518\n",
      "Epoch [420/1000], Loss: 1.9341582457224529\n",
      "Epoch [421/1000], Loss: 1.9307804107666016\n",
      "Epoch [422/1000], Loss: 1.9225447575251262\n",
      "Epoch [423/1000], Loss: 1.912877877553304\n",
      "Epoch [424/1000], Loss: 1.9256692131360371\n",
      "Epoch [425/1000], Loss: 1.8959470987319946\n",
      "Epoch [426/1000], Loss: 1.8951745827992756\n",
      "Epoch [427/1000], Loss: 1.881443738937378\n",
      "Epoch [428/1000], Loss: 1.8840346137682598\n",
      "Epoch [429/1000], Loss: 1.8719255129496257\n",
      "Epoch [430/1000], Loss: 1.8631537357966106\n",
      "Epoch [431/1000], Loss: 1.855686068534851\n",
      "Epoch [432/1000], Loss: 1.8490175406138103\n",
      "Epoch [433/1000], Loss: 1.8380998373031616\n",
      "Epoch [434/1000], Loss: 1.8405763705571492\n",
      "Epoch [435/1000], Loss: 1.829659104347229\n",
      "Epoch [436/1000], Loss: 1.8235232830047607\n",
      "Epoch [437/1000], Loss: 1.8103169997533162\n",
      "Epoch [438/1000], Loss: 1.8034975131352742\n",
      "Epoch [439/1000], Loss: 1.8081013162930806\n",
      "Epoch [440/1000], Loss: 1.7921934127807617\n",
      "Epoch [441/1000], Loss: 1.7847096522649128\n",
      "Epoch [442/1000], Loss: 1.7753242254257202\n",
      "Epoch [443/1000], Loss: 1.7680914402008057\n",
      "Epoch [444/1000], Loss: 1.763021965821584\n",
      "Epoch [445/1000], Loss: 1.756309191385905\n",
      "Epoch [446/1000], Loss: 1.7481119235356648\n",
      "Epoch [447/1000], Loss: 1.7409695386886597\n",
      "Epoch [448/1000], Loss: 1.7298582394917805\n",
      "Epoch [449/1000], Loss: 1.7272203365961711\n",
      "Epoch [450/1000], Loss: 1.7232418060302734\n",
      "Epoch [451/1000], Loss: 1.7117276191711426\n",
      "Epoch [452/1000], Loss: 1.7123050491015117\n",
      "Epoch [453/1000], Loss: 1.70255047082901\n",
      "Epoch [454/1000], Loss: 1.68822846810023\n",
      "Epoch [455/1000], Loss: 1.680912693341573\n",
      "Epoch [456/1000], Loss: 1.676644206047058\n",
      "Epoch [457/1000], Loss: 1.6666622559229534\n",
      "Epoch [458/1000], Loss: 1.6645750602086384\n",
      "Epoch [459/1000], Loss: 1.659194032351176\n",
      "Epoch [460/1000], Loss: 1.6485317548116047\n",
      "Epoch [461/1000], Loss: 1.642128864924113\n",
      "Epoch [462/1000], Loss: 1.6324466466903687\n",
      "Epoch [463/1000], Loss: 1.6249961455663045\n",
      "Epoch [464/1000], Loss: 1.620721975962321\n",
      "Epoch [465/1000], Loss: 1.6160120169321697\n",
      "Epoch [466/1000], Loss: 1.6092522342999775\n",
      "Epoch [467/1000], Loss: 1.6014796495437622\n",
      "Epoch [468/1000], Loss: 1.5921685695648193\n",
      "Epoch [469/1000], Loss: 1.5942702293395996\n",
      "Epoch [470/1000], Loss: 1.5853447715441387\n",
      "Epoch [471/1000], Loss: 1.5711383422215779\n",
      "Epoch [472/1000], Loss: 1.5670862197875977\n",
      "Epoch [473/1000], Loss: 1.557538390159607\n",
      "Epoch [474/1000], Loss: 1.5534375508626301\n",
      "Epoch [475/1000], Loss: 1.5431336561838787\n",
      "Epoch [476/1000], Loss: 1.5396997133890789\n",
      "Epoch [477/1000], Loss: 1.5340537627538045\n",
      "Epoch [478/1000], Loss: 1.5264345010121663\n",
      "Epoch [479/1000], Loss: 1.5224937001864116\n",
      "Epoch [480/1000], Loss: 1.5149857600529988\n",
      "Epoch [481/1000], Loss: 1.5071003834406536\n",
      "Epoch [482/1000], Loss: 1.4964463313420613\n",
      "Epoch [483/1000], Loss: 1.493111213048299\n",
      "Epoch [484/1000], Loss: 1.4836679697036743\n",
      "Epoch [485/1000], Loss: 1.4798208077748616\n",
      "Epoch [486/1000], Loss: 1.472948431968689\n",
      "Epoch [487/1000], Loss: 1.4758557081222534\n",
      "Epoch [488/1000], Loss: 1.4585692485173543\n",
      "Epoch [489/1000], Loss: 1.4511419932047527\n",
      "Epoch [490/1000], Loss: 1.4519999225934346\n",
      "Epoch [491/1000], Loss: 1.4406879544258118\n",
      "Epoch [492/1000], Loss: 1.431172211964925\n",
      "Epoch [493/1000], Loss: 1.4288322726885478\n",
      "Epoch [494/1000], Loss: 1.4175018469492595\n",
      "Epoch [495/1000], Loss: 1.4109859069188435\n",
      "Epoch [496/1000], Loss: 1.407817006111145\n",
      "Epoch [497/1000], Loss: 1.4001219868659973\n",
      "Epoch [498/1000], Loss: 1.4044590791066487\n",
      "Epoch [499/1000], Loss: 1.3929734627405803\n",
      "Epoch [500/1000], Loss: 1.3831186294555664\n",
      "Epoch [501/1000], Loss: 1.379414935906728\n",
      "Epoch [502/1000], Loss: 1.3664303819338481\n",
      "Epoch [503/1000], Loss: 1.3656593561172485\n",
      "Epoch [504/1000], Loss: 1.358800192674001\n",
      "Epoch [505/1000], Loss: 1.3519078890482585\n",
      "Epoch [506/1000], Loss: 1.34536608060201\n",
      "Epoch [507/1000], Loss: 1.3489343325297039\n",
      "Epoch [508/1000], Loss: 1.328621784845988\n",
      "Epoch [509/1000], Loss: 1.3240784804026287\n",
      "Epoch [510/1000], Loss: 1.3170981804529827\n",
      "Epoch [511/1000], Loss: 1.3213523427645366\n",
      "Epoch [512/1000], Loss: 1.303701897462209\n",
      "Epoch [513/1000], Loss: 1.2975134054819744\n",
      "Epoch [514/1000], Loss: 1.2941807508468628\n",
      "Epoch [515/1000], Loss: 1.2869974970817566\n",
      "Epoch [516/1000], Loss: 1.285900314648946\n",
      "Epoch [517/1000], Loss: 1.2728974024454753\n",
      "Epoch [518/1000], Loss: 1.267781178156535\n",
      "Epoch [519/1000], Loss: 1.2638166348139446\n",
      "Epoch [520/1000], Loss: 1.2594666481018066\n",
      "Epoch [521/1000], Loss: 1.2515406012535095\n",
      "Epoch [522/1000], Loss: 1.2424486478169758\n",
      "Epoch [523/1000], Loss: 1.244076172510783\n",
      "Epoch [524/1000], Loss: 1.2344143788019817\n",
      "Epoch [525/1000], Loss: 1.2276039918263753\n",
      "Epoch [526/1000], Loss: 1.222465733687083\n",
      "Epoch [527/1000], Loss: 1.2202890515327454\n",
      "Epoch [528/1000], Loss: 1.2080163359642029\n",
      "Epoch [529/1000], Loss: 1.2137487133344014\n",
      "Epoch [530/1000], Loss: 1.2083608706792195\n",
      "Epoch [531/1000], Loss: 1.1969109972318013\n",
      "Epoch [532/1000], Loss: 1.185433526833852\n",
      "Epoch [533/1000], Loss: 1.1782448689142864\n",
      "Epoch [534/1000], Loss: 1.174970308939616\n",
      "Epoch [535/1000], Loss: 1.1675525307655334\n",
      "Epoch [536/1000], Loss: 1.1633309920628865\n",
      "Epoch [537/1000], Loss: 1.158211628595988\n",
      "Epoch [538/1000], Loss: 1.1491435766220093\n",
      "Epoch [539/1000], Loss: 1.1462171077728271\n",
      "Epoch [540/1000], Loss: 1.138795534769694\n",
      "Epoch [541/1000], Loss: 1.1320734818776448\n",
      "Epoch [542/1000], Loss: 1.1266204913457234\n",
      "Epoch [543/1000], Loss: 1.1316781640052795\n",
      "Epoch [544/1000], Loss: 1.1154516339302063\n",
      "Epoch [545/1000], Loss: 1.1125784516334534\n",
      "Epoch [546/1000], Loss: 1.1114058295885723\n",
      "Epoch [547/1000], Loss: 1.1016708215077717\n",
      "Epoch [548/1000], Loss: 1.1020100712776184\n",
      "Epoch [549/1000], Loss: 1.0887850125630696\n",
      "Epoch [550/1000], Loss: 1.085111955801646\n",
      "Epoch [551/1000], Loss: 1.0807850360870361\n",
      "Epoch [552/1000], Loss: 1.0712965726852417\n",
      "Epoch [553/1000], Loss: 1.069386879603068\n",
      "Epoch [554/1000], Loss: 1.061896522839864\n",
      "Epoch [555/1000], Loss: 1.0551695426305134\n",
      "Epoch [556/1000], Loss: 1.0587793191274006\n",
      "Epoch [557/1000], Loss: 1.045103947321574\n",
      "Epoch [558/1000], Loss: 1.0412312348683674\n",
      "Epoch [559/1000], Loss: 1.0361991723378499\n",
      "Epoch [560/1000], Loss: 1.0415250062942505\n",
      "Epoch [561/1000], Loss: 1.0360881288846333\n",
      "Epoch [562/1000], Loss: 1.0221129655838013\n",
      "Epoch [563/1000], Loss: 1.0151259899139404\n",
      "Epoch [564/1000], Loss: 1.010155936082204\n",
      "Epoch [565/1000], Loss: 1.0055530269940693\n",
      "Epoch [566/1000], Loss: 1.0079145630200703\n",
      "Epoch [567/1000], Loss: 1.0005742112795513\n",
      "Epoch [568/1000], Loss: 0.9933322270711263\n",
      "Epoch [569/1000], Loss: 0.9872663815816244\n",
      "Epoch [570/1000], Loss: 0.9843220412731171\n",
      "Epoch [571/1000], Loss: 0.9803977409998575\n",
      "Epoch [572/1000], Loss: 0.9710345268249512\n",
      "Epoch [573/1000], Loss: 0.9637863039970398\n",
      "Epoch [574/1000], Loss: 0.9643730719884237\n",
      "Epoch [575/1000], Loss: 0.9536703824996948\n",
      "Epoch [576/1000], Loss: 0.9491498668988546\n",
      "Epoch [577/1000], Loss: 0.9459196329116821\n",
      "Epoch [578/1000], Loss: 0.9433610439300537\n",
      "Epoch [579/1000], Loss: 0.9392759601275126\n",
      "Epoch [580/1000], Loss: 0.9392877022425333\n",
      "Epoch [581/1000], Loss: 0.9281394282976786\n",
      "Epoch [582/1000], Loss: 0.9219139417012533\n",
      "Epoch [583/1000], Loss: 0.9191471536954244\n",
      "Epoch [584/1000], Loss: 0.9126339356104533\n",
      "Epoch [585/1000], Loss: 0.9086079994837443\n",
      "Epoch [586/1000], Loss: 0.9050597151120504\n",
      "Epoch [587/1000], Loss: 0.9009506901105245\n",
      "Epoch [588/1000], Loss: 0.9048293034235636\n",
      "Epoch [589/1000], Loss: 0.8902547756830851\n",
      "Epoch [590/1000], Loss: 0.8865399559338888\n",
      "Epoch [591/1000], Loss: 0.8919859925905863\n",
      "Epoch [592/1000], Loss: 0.8776666522026062\n",
      "Epoch [593/1000], Loss: 0.8730807900428772\n",
      "Epoch [594/1000], Loss: 0.866709848244985\n",
      "Epoch [595/1000], Loss: 0.8644258181254069\n",
      "Epoch [596/1000], Loss: 0.8621232708295187\n",
      "Epoch [597/1000], Loss: 0.856943686803182\n",
      "Epoch [598/1000], Loss: 0.8497297565142313\n",
      "Epoch [599/1000], Loss: 0.8630287746588389\n",
      "Epoch [600/1000], Loss: 0.8436286449432373\n",
      "Epoch [601/1000], Loss: 0.8399677077929179\n",
      "Epoch [602/1000], Loss: 0.8393186728159586\n",
      "Epoch [603/1000], Loss: 0.831039567788442\n",
      "Epoch [604/1000], Loss: 0.8272751967112223\n",
      "Epoch [605/1000], Loss: 0.8222801089286804\n",
      "Epoch [606/1000], Loss: 0.8201395273208618\n",
      "Epoch [607/1000], Loss: 0.814158042271932\n",
      "Epoch [608/1000], Loss: 0.8137251635392507\n",
      "Epoch [609/1000], Loss: 0.8128332793712616\n",
      "Epoch [610/1000], Loss: 0.8030171593030294\n",
      "Epoch [611/1000], Loss: 0.799312969048818\n",
      "Epoch [612/1000], Loss: 0.795077363650004\n",
      "Epoch [613/1000], Loss: 0.7920565406481425\n",
      "Epoch [614/1000], Loss: 0.8007655739784241\n",
      "Epoch [615/1000], Loss: 0.7879013816515604\n",
      "Epoch [616/1000], Loss: 0.7806708614031473\n",
      "Epoch [617/1000], Loss: 0.7761392990748087\n",
      "Epoch [618/1000], Loss: 0.7705657680829366\n",
      "Epoch [619/1000], Loss: 0.7697696487108866\n",
      "Epoch [620/1000], Loss: 0.7632132371266683\n",
      "Epoch [621/1000], Loss: 0.7610727151234945\n",
      "Epoch [622/1000], Loss: 0.7582646012306213\n",
      "Epoch [623/1000], Loss: 0.7577442129453024\n",
      "Epoch [624/1000], Loss: 0.7513323426246643\n",
      "Epoch [625/1000], Loss: 0.7454189658164978\n",
      "Epoch [626/1000], Loss: 0.7441076040267944\n",
      "Epoch [627/1000], Loss: 0.7385239402453104\n",
      "Epoch [628/1000], Loss: 0.7366929054260254\n",
      "Epoch [629/1000], Loss: 0.7317335406939188\n",
      "Epoch [630/1000], Loss: 0.7308271725972494\n",
      "Epoch [631/1000], Loss: 0.7315712571144104\n",
      "Epoch [632/1000], Loss: 0.7216091354688009\n",
      "Epoch [633/1000], Loss: 0.7204927404721578\n",
      "Epoch [634/1000], Loss: 0.7168602148691813\n",
      "Epoch [635/1000], Loss: 0.7117003599802653\n",
      "Epoch [636/1000], Loss: 0.7104219794273376\n",
      "Epoch [637/1000], Loss: 0.7072347203890482\n",
      "Epoch [638/1000], Loss: 0.7039274175961813\n",
      "Epoch [639/1000], Loss: 0.7012872695922852\n",
      "Epoch [640/1000], Loss: 0.7032647132873535\n",
      "Epoch [641/1000], Loss: 0.6947381496429443\n",
      "Epoch [642/1000], Loss: 0.6918472846349081\n",
      "Epoch [643/1000], Loss: 0.6868054072062174\n",
      "Epoch [644/1000], Loss: 0.68638147910436\n",
      "Epoch [645/1000], Loss: 0.6806154052416483\n",
      "Epoch [646/1000], Loss: 0.6794451077779134\n",
      "Epoch [647/1000], Loss: 0.6768107612927755\n",
      "Epoch [648/1000], Loss: 0.6744068662325541\n",
      "Epoch [649/1000], Loss: 0.6688575943311056\n",
      "Epoch [650/1000], Loss: 0.6679116090138754\n",
      "Epoch [651/1000], Loss: 0.6650869448979696\n",
      "Epoch [652/1000], Loss: 0.6617814103762308\n",
      "Epoch [653/1000], Loss: 0.6596087217330933\n",
      "Epoch [654/1000], Loss: 0.662546326716741\n",
      "Epoch [655/1000], Loss: 0.6539997458457947\n",
      "Epoch [656/1000], Loss: 0.6509948571523031\n",
      "Epoch [657/1000], Loss: 0.6513752341270447\n",
      "Epoch [658/1000], Loss: 0.6456284721692404\n",
      "Epoch [659/1000], Loss: 0.6428232192993164\n",
      "Epoch [660/1000], Loss: 0.6436912814776102\n",
      "Epoch [661/1000], Loss: 0.6358475089073181\n",
      "Epoch [662/1000], Loss: 0.6332433621088663\n",
      "Epoch [663/1000], Loss: 0.632583737373352\n",
      "Epoch [664/1000], Loss: 0.6281128327051798\n",
      "Epoch [665/1000], Loss: 0.6273421247800192\n",
      "Epoch [666/1000], Loss: 0.623128334681193\n",
      "Epoch [667/1000], Loss: 0.6224579215049744\n",
      "Epoch [668/1000], Loss: 0.6198214491208395\n",
      "Epoch [669/1000], Loss: 0.6210719048976898\n",
      "Epoch [670/1000], Loss: 0.6133860548337301\n",
      "Epoch [671/1000], Loss: 0.6125204066435496\n",
      "Epoch [672/1000], Loss: 0.6133596400419871\n",
      "Epoch [673/1000], Loss: 0.6074918111165365\n",
      "Epoch [674/1000], Loss: 0.6037172277768453\n",
      "Epoch [675/1000], Loss: 0.6032415131727854\n",
      "Epoch [676/1000], Loss: 0.604069322347641\n",
      "Epoch [677/1000], Loss: 0.5983016689618429\n",
      "Epoch [678/1000], Loss: 0.5963082710901896\n",
      "Epoch [679/1000], Loss: 0.5922521750132242\n",
      "Epoch [680/1000], Loss: 0.5917558868726095\n",
      "Epoch [681/1000], Loss: 0.5877697070439657\n",
      "Epoch [682/1000], Loss: 0.5873759686946869\n",
      "Epoch [683/1000], Loss: 0.5835198561350504\n",
      "Epoch [684/1000], Loss: 0.5827924211819967\n",
      "Epoch [685/1000], Loss: 0.5791679422060648\n",
      "Epoch [686/1000], Loss: 0.5815961261590322\n",
      "Epoch [687/1000], Loss: 0.5765255490938822\n",
      "Epoch [688/1000], Loss: 0.5743233263492584\n",
      "Epoch [689/1000], Loss: 0.5724900563557943\n",
      "Epoch [690/1000], Loss: 0.5702878137429556\n",
      "Epoch [691/1000], Loss: 0.568124790986379\n",
      "Epoch [692/1000], Loss: 0.564715584119161\n",
      "Epoch [693/1000], Loss: 0.5642274220784506\n",
      "Epoch [694/1000], Loss: 0.5624019304911295\n",
      "Epoch [695/1000], Loss: 0.5587844649950663\n",
      "Epoch [696/1000], Loss: 0.5569217999776205\n",
      "Epoch [697/1000], Loss: 0.5562868018945059\n",
      "Epoch [698/1000], Loss: 0.5530248681704203\n",
      "Epoch [699/1000], Loss: 0.5526590744654337\n",
      "Epoch [700/1000], Loss: 0.5492254694302877\n",
      "Epoch [701/1000], Loss: 0.5537190834681193\n",
      "Epoch [702/1000], Loss: 0.5468807617823283\n",
      "Epoch [703/1000], Loss: 0.5449545780817667\n",
      "Epoch [704/1000], Loss: 0.5433173179626465\n",
      "Epoch [705/1000], Loss: 0.5418765644232432\n",
      "Epoch [706/1000], Loss: 0.5399969816207886\n",
      "Epoch [707/1000], Loss: 0.5382116436958313\n",
      "Epoch [708/1000], Loss: 0.5389142334461212\n",
      "Epoch [709/1000], Loss: 0.5343052744865417\n",
      "Epoch [710/1000], Loss: 0.5324708918730418\n",
      "Epoch [711/1000], Loss: 0.5297258794307709\n",
      "Epoch [712/1000], Loss: 0.529060572385788\n",
      "Epoch [713/1000], Loss: 0.5278325974941254\n",
      "Epoch [714/1000], Loss: 0.5261829495429993\n",
      "Epoch [715/1000], Loss: 0.5274271766344706\n",
      "Epoch [716/1000], Loss: 0.5223761200904846\n",
      "Epoch [717/1000], Loss: 0.5211344162623087\n",
      "Epoch [718/1000], Loss: 0.5196595092614492\n",
      "Epoch [719/1000], Loss: 0.5165190895398458\n",
      "Epoch [720/1000], Loss: 0.5149334073066711\n",
      "Epoch [721/1000], Loss: 0.5147756139437357\n",
      "Epoch [722/1000], Loss: 0.5136281649271647\n",
      "Epoch [723/1000], Loss: 0.5119027098019918\n",
      "Epoch [724/1000], Loss: 0.510204941034317\n",
      "Epoch [725/1000], Loss: 0.5087601542472839\n",
      "Epoch [726/1000], Loss: 0.5111089050769806\n",
      "Epoch [727/1000], Loss: 0.505607416232427\n",
      "Epoch [728/1000], Loss: 0.5080017944176992\n",
      "Epoch [729/1000], Loss: 0.5030023654301962\n",
      "Epoch [730/1000], Loss: 0.5012184878190359\n",
      "Epoch [731/1000], Loss: 0.4996252457300822\n",
      "Epoch [732/1000], Loss: 0.49701714515686035\n",
      "Epoch [733/1000], Loss: 0.495697557926178\n",
      "Epoch [734/1000], Loss: 0.4952166676521301\n",
      "Epoch [735/1000], Loss: 0.49373284975687665\n",
      "Epoch [736/1000], Loss: 0.4921397864818573\n",
      "Epoch [737/1000], Loss: 0.49559346834818524\n",
      "Epoch [738/1000], Loss: 0.49020518859227497\n",
      "Epoch [739/1000], Loss: 0.4874328374862671\n",
      "Epoch [740/1000], Loss: 0.48703580101331073\n",
      "Epoch [741/1000], Loss: 0.4855796198050181\n",
      "Epoch [742/1000], Loss: 0.48452599843343097\n",
      "Epoch [743/1000], Loss: 0.48217831055323285\n",
      "Epoch [744/1000], Loss: 0.48086922367413837\n",
      "Epoch [745/1000], Loss: 0.4843706687291463\n",
      "Epoch [746/1000], Loss: 0.4823550780614217\n",
      "Epoch [747/1000], Loss: 0.4775826434294383\n",
      "Epoch [748/1000], Loss: 0.4793620804945628\n",
      "Epoch [749/1000], Loss: 0.4764599601427714\n",
      "Epoch [750/1000], Loss: 0.4742734332879384\n",
      "Epoch [751/1000], Loss: 0.47300923864046734\n",
      "Epoch [752/1000], Loss: 0.47229232390721637\n",
      "Epoch [753/1000], Loss: 0.46967082222302753\n",
      "Epoch [754/1000], Loss: 0.46851080656051636\n",
      "Epoch [755/1000], Loss: 0.46732810139656067\n",
      "Epoch [756/1000], Loss: 0.46706511576970416\n",
      "Epoch [757/1000], Loss: 0.46601494153340656\n",
      "Epoch [758/1000], Loss: 0.4669099549452464\n",
      "Epoch [759/1000], Loss: 0.46415000160535175\n",
      "Epoch [760/1000], Loss: 0.46538312236468\n",
      "Epoch [761/1000], Loss: 0.4611639479796092\n",
      "Epoch [762/1000], Loss: 0.46233275532722473\n",
      "Epoch [763/1000], Loss: 0.458346962928772\n",
      "Epoch [764/1000], Loss: 0.46179768443107605\n",
      "Epoch [765/1000], Loss: 0.4561724265416463\n",
      "Epoch [766/1000], Loss: 0.4566696286201477\n",
      "Epoch [767/1000], Loss: 0.4548121591409047\n",
      "Epoch [768/1000], Loss: 0.4542352060476939\n",
      "Epoch [769/1000], Loss: 0.4532965421676636\n",
      "Epoch [770/1000], Loss: 0.45520291725794476\n",
      "Epoch [771/1000], Loss: 0.45324573914210003\n",
      "Epoch [772/1000], Loss: 0.45053668816884357\n",
      "Epoch [773/1000], Loss: 0.4480649530887604\n",
      "Epoch [774/1000], Loss: 0.448929101228714\n",
      "Epoch [775/1000], Loss: 0.4465861916542053\n",
      "Epoch [776/1000], Loss: 0.4458504617214203\n",
      "Epoch [777/1000], Loss: 0.4439695477485657\n",
      "Epoch [778/1000], Loss: 0.4438210229078929\n",
      "Epoch [779/1000], Loss: 0.4460839827855428\n",
      "Epoch [780/1000], Loss: 0.4421190619468689\n",
      "Epoch [781/1000], Loss: 0.4410394529501597\n",
      "Epoch [782/1000], Loss: 0.43898998697598773\n",
      "Epoch [783/1000], Loss: 0.43870115280151367\n",
      "Epoch [784/1000], Loss: 0.43695292870203656\n",
      "Epoch [785/1000], Loss: 0.43794461091359455\n",
      "Epoch [786/1000], Loss: 0.4362168610095978\n",
      "Epoch [787/1000], Loss: 0.43464625875155133\n",
      "Epoch [788/1000], Loss: 0.43421513835589093\n",
      "Epoch [789/1000], Loss: 0.4356280565261841\n",
      "Epoch [790/1000], Loss: 0.4315481185913086\n",
      "Epoch [791/1000], Loss: 0.43062520027160645\n",
      "Epoch [792/1000], Loss: 0.4296633501847585\n",
      "Epoch [793/1000], Loss: 0.4288370907306671\n",
      "Epoch [794/1000], Loss: 0.42797115445137024\n",
      "Epoch [795/1000], Loss: 0.42730340361595154\n",
      "Epoch [796/1000], Loss: 0.4294076959292094\n",
      "Epoch [797/1000], Loss: 0.42593804001808167\n",
      "Epoch [798/1000], Loss: 0.4245084822177887\n",
      "Epoch [799/1000], Loss: 0.42463502784570056\n",
      "Epoch [800/1000], Loss: 0.42626360058784485\n",
      "Epoch [801/1000], Loss: 0.42255820830663043\n",
      "Epoch [802/1000], Loss: 0.42143786946932477\n",
      "Epoch [803/1000], Loss: 0.42130376895268756\n",
      "Epoch [804/1000], Loss: 0.420096496740977\n",
      "Epoch [805/1000], Loss: 0.4195335606733958\n",
      "Epoch [806/1000], Loss: 0.41914424300193787\n",
      "Epoch [807/1000], Loss: 0.4212842285633087\n",
      "Epoch [808/1000], Loss: 0.4165154993534088\n",
      "Epoch [809/1000], Loss: 0.41675833861033124\n",
      "Epoch [810/1000], Loss: 0.41490015387535095\n",
      "Epoch [811/1000], Loss: 0.4154890676339467\n",
      "Epoch [812/1000], Loss: 0.41345367829004925\n",
      "Epoch [813/1000], Loss: 0.4131087263425191\n",
      "Epoch [814/1000], Loss: 0.41248150666554767\n",
      "Epoch [815/1000], Loss: 0.41241832574208576\n",
      "Epoch [816/1000], Loss: 0.41170435150464374\n",
      "Epoch [817/1000], Loss: 0.41339034338792163\n",
      "Epoch [818/1000], Loss: 0.4092353681723277\n",
      "Epoch [819/1000], Loss: 0.41199424862861633\n",
      "Epoch [820/1000], Loss: 0.4085284074147542\n",
      "Epoch [821/1000], Loss: 0.4067474802335103\n",
      "Epoch [822/1000], Loss: 0.40837132930755615\n",
      "Epoch [823/1000], Loss: 0.40613558888435364\n",
      "Epoch [824/1000], Loss: 0.40578988194465637\n",
      "Epoch [825/1000], Loss: 0.40596339106559753\n",
      "Epoch [826/1000], Loss: 0.4036429027716319\n",
      "Epoch [827/1000], Loss: 0.40337347984313965\n",
      "Epoch [828/1000], Loss: 0.4025029043356578\n",
      "Epoch [829/1000], Loss: 0.4039871394634247\n",
      "Epoch [830/1000], Loss: 0.4013025164604187\n",
      "Epoch [831/1000], Loss: 0.39986300468444824\n",
      "Epoch [832/1000], Loss: 0.4020755688349406\n",
      "Epoch [833/1000], Loss: 0.399030198653539\n",
      "Epoch [834/1000], Loss: 0.3986586232980092\n",
      "Epoch [835/1000], Loss: 0.39966512223084766\n",
      "Epoch [836/1000], Loss: 0.39732518792152405\n",
      "Epoch [837/1000], Loss: 0.3978845278422038\n",
      "Epoch [838/1000], Loss: 0.39538989464441937\n",
      "Epoch [839/1000], Loss: 0.39467095335324603\n",
      "Epoch [840/1000], Loss: 0.3940443197886149\n",
      "Epoch [841/1000], Loss: 0.39415061473846436\n",
      "Epoch [842/1000], Loss: 0.39287225405375165\n",
      "Epoch [843/1000], Loss: 0.3935275773207347\n",
      "Epoch [844/1000], Loss: 0.3960300286610921\n",
      "Epoch [845/1000], Loss: 0.39210421840349835\n",
      "Epoch [846/1000], Loss: 0.3906930685043335\n",
      "Epoch [847/1000], Loss: 0.3906119068463643\n",
      "Epoch [848/1000], Loss: 0.39114659031232196\n",
      "Epoch [849/1000], Loss: 0.38884740074475604\n",
      "Epoch [850/1000], Loss: 0.3883499950170517\n",
      "Epoch [851/1000], Loss: 0.38760894536972046\n",
      "Epoch [852/1000], Loss: 0.3884596526622772\n",
      "Epoch [853/1000], Loss: 0.38653519252936047\n",
      "Epoch [854/1000], Loss: 0.3863434096177419\n",
      "Epoch [855/1000], Loss: 0.3870143194993337\n",
      "Epoch [856/1000], Loss: 0.38721583286921185\n",
      "Epoch [857/1000], Loss: 0.3839555084705353\n",
      "Epoch [858/1000], Loss: 0.3845890859762828\n",
      "Epoch [859/1000], Loss: 0.38385335604349774\n",
      "Epoch [860/1000], Loss: 0.3828589816888173\n",
      "Epoch [861/1000], Loss: 0.3820294141769409\n",
      "Epoch [862/1000], Loss: 0.38160964846611023\n",
      "Epoch [863/1000], Loss: 0.381820688645045\n",
      "Epoch [864/1000], Loss: 0.3800426224867503\n",
      "Epoch [865/1000], Loss: 0.38027694324652356\n",
      "Epoch [866/1000], Loss: 0.37955283125241596\n",
      "Epoch [867/1000], Loss: 0.378803292910258\n",
      "Epoch [868/1000], Loss: 0.3782592713832855\n",
      "Epoch [869/1000], Loss: 0.3781708578268687\n",
      "Epoch [870/1000], Loss: 0.37702850500742596\n",
      "Epoch [871/1000], Loss: 0.3764568666617076\n",
      "Epoch [872/1000], Loss: 0.37707112232844037\n",
      "Epoch [873/1000], Loss: 0.37795331080754596\n",
      "Epoch [874/1000], Loss: 0.3763497869173686\n",
      "Epoch [875/1000], Loss: 0.37657206257184345\n",
      "Epoch [876/1000], Loss: 0.3743203779061635\n",
      "Epoch [877/1000], Loss: 0.3751374731461207\n",
      "Epoch [878/1000], Loss: 0.37326574325561523\n",
      "Epoch [879/1000], Loss: 0.3731173574924469\n",
      "Epoch [880/1000], Loss: 0.3718900978565216\n",
      "Epoch [881/1000], Loss: 0.3721025288105011\n",
      "Epoch [882/1000], Loss: 0.37087758382161456\n",
      "Epoch [883/1000], Loss: 0.3725964625676473\n",
      "Epoch [884/1000], Loss: 0.37252818544705707\n",
      "Epoch [885/1000], Loss: 0.3697468042373657\n",
      "Epoch [886/1000], Loss: 0.3695127069950104\n",
      "Epoch [887/1000], Loss: 0.3688015143076579\n",
      "Epoch [888/1000], Loss: 0.36899756888548535\n",
      "Epoch [889/1000], Loss: 0.3678720494111379\n",
      "Epoch [890/1000], Loss: 0.36719311277071637\n",
      "Epoch [891/1000], Loss: 0.3671986411015193\n",
      "Epoch [892/1000], Loss: 0.3662874698638916\n",
      "Epoch [893/1000], Loss: 0.36748626331488293\n",
      "Epoch [894/1000], Loss: 0.365578552087148\n",
      "Epoch [895/1000], Loss: 0.366357425848643\n",
      "Epoch [896/1000], Loss: 0.36456817885239917\n",
      "Epoch [897/1000], Loss: 0.3642413715521495\n",
      "Epoch [898/1000], Loss: 0.3653077979882558\n",
      "Epoch [899/1000], Loss: 0.36345909039179486\n",
      "Epoch [900/1000], Loss: 0.36272621154785156\n",
      "Epoch [901/1000], Loss: 0.3624420017004013\n",
      "Epoch [902/1000], Loss: 0.36244316895802814\n",
      "Epoch [903/1000], Loss: 0.3618517816066742\n",
      "Epoch [904/1000], Loss: 0.3611227472623189\n",
      "Epoch [905/1000], Loss: 0.36070839564005536\n",
      "Epoch [906/1000], Loss: 0.3601636091868083\n",
      "Epoch [907/1000], Loss: 0.3609733581542969\n",
      "Epoch [908/1000], Loss: 0.3617104987303416\n",
      "Epoch [909/1000], Loss: 0.35995297133922577\n",
      "Epoch [910/1000], Loss: 0.35868845383326214\n",
      "Epoch [911/1000], Loss: 0.35959596435228985\n",
      "Epoch [912/1000], Loss: 0.35785019894440967\n",
      "Epoch [913/1000], Loss: 0.35768656929334003\n",
      "Epoch [914/1000], Loss: 0.35752837856610614\n",
      "Epoch [915/1000], Loss: 0.3573208103577296\n",
      "Epoch [916/1000], Loss: 0.35604043304920197\n",
      "Epoch [917/1000], Loss: 0.35578786333401996\n",
      "Epoch [918/1000], Loss: 0.3554531733194987\n",
      "Epoch [919/1000], Loss: 0.3550591667493184\n",
      "Epoch [920/1000], Loss: 0.35489123066266376\n",
      "Epoch [921/1000], Loss: 0.35403867562611896\n",
      "Epoch [922/1000], Loss: 0.35547202825546265\n",
      "Epoch [923/1000], Loss: 0.3548754056294759\n",
      "Epoch [924/1000], Loss: 0.3538427948951721\n",
      "Epoch [925/1000], Loss: 0.35254573325316113\n",
      "Epoch [926/1000], Loss: 0.35352827111879986\n",
      "Epoch [927/1000], Loss: 0.3519822110732396\n",
      "Epoch [928/1000], Loss: 0.35206080973148346\n",
      "Epoch [929/1000], Loss: 0.35104207197825116\n",
      "Epoch [930/1000], Loss: 0.3522207935651143\n",
      "Epoch [931/1000], Loss: 0.3518565793832143\n",
      "Epoch [932/1000], Loss: 0.3523159424463908\n",
      "Epoch [933/1000], Loss: 0.3502243906259537\n",
      "Epoch [934/1000], Loss: 0.34939878185590106\n",
      "Epoch [935/1000], Loss: 0.35039469599723816\n",
      "Epoch [936/1000], Loss: 0.3524457514286041\n",
      "Epoch [937/1000], Loss: 0.3483378489812215\n",
      "Epoch [938/1000], Loss: 0.3479151626427968\n",
      "Epoch [939/1000], Loss: 0.34809041023254395\n",
      "Epoch [940/1000], Loss: 0.3474406550327937\n",
      "Epoch [941/1000], Loss: 0.34686288237571716\n",
      "Epoch [942/1000], Loss: 0.347622608145078\n",
      "Epoch [943/1000], Loss: 0.3466614782810211\n",
      "Epoch [944/1000], Loss: 0.35013604164123535\n",
      "Epoch [945/1000], Loss: 0.34652013580004376\n",
      "Epoch [946/1000], Loss: 0.34585397442181903\n",
      "Epoch [947/1000], Loss: 0.3468116323153178\n",
      "Epoch [948/1000], Loss: 0.3443995813528697\n",
      "Epoch [949/1000], Loss: 0.34406063954035443\n",
      "Epoch [950/1000], Loss: 0.3444909652074178\n",
      "Epoch [951/1000], Loss: 0.3445009191830953\n",
      "Epoch [952/1000], Loss: 0.3458545506000519\n",
      "Epoch [953/1000], Loss: 0.3430752158164978\n",
      "Epoch [954/1000], Loss: 0.3427356779575348\n",
      "Epoch [955/1000], Loss: 0.34208063284556073\n",
      "Epoch [956/1000], Loss: 0.3443186084429423\n",
      "Epoch [957/1000], Loss: 0.3419756442308426\n",
      "Epoch [958/1000], Loss: 0.3410051961739858\n",
      "Epoch [959/1000], Loss: 0.34068766236305237\n",
      "Epoch [960/1000], Loss: 0.34111188352108\n",
      "Epoch [961/1000], Loss: 0.34020479520161945\n",
      "Epoch [962/1000], Loss: 0.34498938421408337\n",
      "Epoch [963/1000], Loss: 0.33961071570714313\n",
      "Epoch [964/1000], Loss: 0.3405658006668091\n",
      "Epoch [965/1000], Loss: 0.34109748403231305\n",
      "Epoch [966/1000], Loss: 0.33866651852925617\n",
      "Epoch [967/1000], Loss: 0.338826189438502\n",
      "Epoch [968/1000], Loss: 0.3381863882144292\n",
      "Epoch [969/1000], Loss: 0.3396979719400406\n",
      "Epoch [970/1000], Loss: 0.33950285116831463\n",
      "Epoch [971/1000], Loss: 0.3378006269534429\n",
      "Epoch [972/1000], Loss: 0.33676796158154804\n",
      "Epoch [973/1000], Loss: 0.3375375767548879\n",
      "Epoch [974/1000], Loss: 0.3364838163057963\n",
      "Epoch [975/1000], Loss: 0.33703309297561646\n",
      "Epoch [976/1000], Loss: 0.3367276191711426\n",
      "Epoch [977/1000], Loss: 0.3352207193771998\n",
      "Epoch [978/1000], Loss: 0.33721648156642914\n",
      "Epoch [979/1000], Loss: 0.33576517800490063\n",
      "Epoch [980/1000], Loss: 0.3350338041782379\n",
      "Epoch [981/1000], Loss: 0.3354761302471161\n",
      "Epoch [982/1000], Loss: 0.3349142024914424\n",
      "Epoch [983/1000], Loss: 0.3336769938468933\n",
      "Epoch [984/1000], Loss: 0.3333071768283844\n",
      "Epoch [985/1000], Loss: 0.3336043059825897\n",
      "Epoch [986/1000], Loss: 0.336191659172376\n",
      "Epoch [987/1000], Loss: 0.3329594333966573\n",
      "Epoch [988/1000], Loss: 0.332246666153272\n",
      "Epoch [989/1000], Loss: 0.33314696451028186\n",
      "Epoch [990/1000], Loss: 0.33192141354084015\n",
      "Epoch [991/1000], Loss: 0.3317505717277527\n",
      "Epoch [992/1000], Loss: 0.3309325675169627\n",
      "Epoch [993/1000], Loss: 0.33568257590134937\n",
      "Epoch [994/1000], Loss: 0.3305991590023041\n",
      "Epoch [995/1000], Loss: 0.3313082158565521\n",
      "Epoch [996/1000], Loss: 0.33253014087677\n",
      "Epoch [997/1000], Loss: 0.330118993918101\n",
      "Epoch [998/1000], Loss: 0.3295234839121501\n",
      "Epoch [999/1000], Loss: 0.3296728531519572\n",
      "Epoch [1000/1000], Loss: 0.32969655593236286\n",
      "Фінальна втрата: 0.32860657572746277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4x/c_v1qrzn0_z359h7ycv_jvsc0000gn/T/ipykernel_88191/3628309636.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  final_predictions = model(torch.tensor(inputs, dtype=torch.float32))\n",
      "/var/folders/4x/c_v1qrzn0_z359h7ycv_jvsc0000gn/T/ipykernel_88191/3628309636.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  final_loss = compute_loss(final_predictions, torch.tensor(targets, dtype=torch.float32))\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train() \n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs_batch, targets_batch in train_loader:\n",
    "        opt.zero_grad()  \n",
    "        \n",
    "        predictions = model(inputs_batch)\n",
    "        \n",
    "        # Обчислюємо втрати\n",
    "        loss = compute_loss(predictions, targets_batch)\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    \n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "model.eval() \n",
    "with torch.no_grad():\n",
    "    final_predictions = model(torch.tensor(inputs, dtype=torch.float32))\n",
    "    final_loss = compute_loss(final_predictions, torch.tensor(targets, dtype=torch.float32))\n",
    "\n",
    "print(f\"Фінальна втрата: {final_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cEHQH9qE626k"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
